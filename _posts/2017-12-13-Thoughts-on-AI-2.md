---
title: "Thoughts on AI 2/2"
layout: post
categories:
  - ZHAW
  - Opinion
draft: false
---

*The following post is an assignment for the ZHAW course KI1 (Artificial Intelligence), given by [Thilo Stadelmann](http://stdm.github.io) during the autumn semester of 2017.*

*This assignment is a follow up exercise, where each student is given the task to write a second blog post and provide a reply to the [first blog post](../Thoughts-on-AI).*

---

A few months ago, I ventured into the first artificial intelligence lecture, only partially knowing what to expect. I assumed I would be confronted with algorithms, problem solving and possibly the need for life insurance. Instead, this semester demonstrated that the name Artificial Intelligene is a catchy yet unfavorable name, while "algorithms for complex computer problems" would have been much less of a hype magnet.

Tim Urban's *[The AI Revolution: The Road to Superintelliengece](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html)* painted a very optimistic "the end is nigh(-ish)" picture with the end of humanity in close proximity. Its writing captures an audience, but how trustworthy is the information given? Recent years have shown that AI programs are capable of solving incredibly complex tasks both faster and better than humans ever could. Adding small adjustments to these problems however would need a complete overhaul for the AI system that solved it. The question we need to ask ourselves is:

*Is it realistic to build an agent that can do everything a human can just by placing every individual function into a box?*

Doing just that wouldn't make an agent human, nor intelligent. Each and every one of those functions (e.g. recognising street signs, understanding what a stop sign means, reacting with an appropriate action, ...) is something that can (given enough time) be solved at least partially with AI algorithms today, but *does that make it intelligent?*

This semester gave me insight into many of the ways AI systems are being used today to achieve certain goals (defeating the Go world champion, evaluating if you should get a loan or not, and many more). What I take from these examples is that the intelligence mostly lies within the developer of the algorithm or agent and not in the application of an algorithm. (Although coming up with an algorithm for such a problem... that needs intelligence.)

Is AlphaGo intelligent, just because it found a move that is neither played by humans nor understandable by master Go players? No. While miraculous to see such new moves being discovered, AlphaGo is mostly a great implementation for searching the movespace of Go.

At the end of this semester, I strongly feel that with every claim for a real *intelligent* AI agent, one must look at it in detail and verify that it is not just another clever algorithm. Which is why I am amused by John McCarthy's quote:

> *"As soon as it works, no one calls it AI anymore."

When reading Tim Urban's article at the beginning of the semester, his claims seemed reasonable and - for an uninformed mind - logical. While the conclusions he draws are still the same, my views have been influenced by the insights gained during the KI1 course. I still believe and to some extent hope that one day there will be an AGI agent, but I am more careful with an estimate. If I learned one thing, the logarithmic scale is *unnatural* und any predictions might fall short like Moore's Law in the recent years.

---

## Sources
[Brooks, 2017] Brooks Rodney, “[The Seven Deadly Sins of AI Predictions](https://www.technologyreview.com/s/609048/the-seven-deadly-sins-of-ai-predictions/)”. 2017.

[Urban, 2015] Tim Urban, “[The AI Revolution: The Road to Superintelligence](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html)”. 2015.
